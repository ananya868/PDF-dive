# -- General Configuration for AI Core Utilities --

# -- LLM General --
llm_general:
  openai:
    default_model: "gpt-4o-mini"
    models: 
      - "gpt-4o"
      - "gpt-4.1"
      - "gpt-4.1-mini"
      - "gpt-4.1-nano"
      - "gpt-4o-mini"
      - "gpt-3.5-turbo"
  google:
    default_model: "gemini-1.5-flash"
    models: 
      - "gemini-2.5-flash"
      - "gemini-2.5-pro"
      - "gemini-2.0-flash"
      - "gemini-2.0-flash-lite"      
      - "gemini-1.5-flash"
      - "gemini-1.5-flash-8b"
      - "gemini-1.5-pro"
  groq:
    default_model: "llama-3.3-70b-versatile"
    models: 
      - "llama-3.1-8b-instant" 
      - "llama-3.3-70b-versatile"
      - "llama3-70b"
      - "llama3-8b"
      - "deepseek-r1-distill-llama-70b"
      - "meta-llama/llama-4-scout-17b-16e-instruct"
  mistral:
    default_model: "mistral-large"
    models:
      - "mistral-small"
      - "mistral-3b"
      - "mistral-8b"
      - "mistral-large"
  anthropic:
    default_model: "claude-4"
    models:
      - "claude-4" 
      - "claude-sonnet-4"
      - "claude-3-7-sonnet"
      - "claude-3-5-haiku"
      - "claude-3-5-sonnet"


# -- LLM Structured --
llm_structured:
  openai:
    default_model: "gpt-4.1-mini"
    models: 
      - "gpt-4o"
      - "gpt-4.1-mini"
      - "gpt-4.1-nano"
      - "gpt-4o-mini"
  google:
    default_model: "gemini-1.5-flash"
    models: 
      - "gemini-2.0-flash-lite"
      - "gemini-1.5-flash"
      - "gemini-1.5-flash-8b"
  groq: 
    default_model: "llama-3.3-70b-versatile"
    models: 
      - "llama-3.1-8b-instant" 
      - "llama-3.3-70b-versatile"
      - "deepseek-r1-distill-llama-70b"
      - "meta-llama/llama-4-scout-17b-16e-instruct"
  mistral:
    default_model: "mistral-small"
    models: 
      - "mistral-small"
      - "mistral-3b"
      - "mistral-8b"
      - "mistral-large"


